\documentclass[a4paper,german]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{hyperref}


%This is how you can create a "claim"-environment (or a lemma/Theorem/definition etc environment)
\newtheorem{claim}{Claim}

%This is how you can create a command of your own, e.g. to simplify the usage signs you often use.
\newcommand{\E}{\mathbb{E}}

%Titlepage:
\title{Solutions of GHW 2}
\author{Luca Di Bartolomeo}
\date{}


\begin{document}
\maketitle

{
	\centering
	Collaborators: Leonardo del Giudice, Anselme Goetschmann

}

\section*{Solution to Exercise 1}

\emph{Explaination:} We are going to use Theorem 9.12 from the lecture notes to show that $E[T] \leq O(n^2)$.


The Markov chain $X(t)$ has $n+1$ states, and only two of them are absorbing (state $X_0$ and state $X_n$).
For all other states $X_i$ with $0 < i < n$, we can use the law of total probability to calculate 
the chance of moving to the right or to the left at each step:
\begin{align*}
	\text{Moving to the left } \;\; p &= \frac{1}{n^4} + \frac{1}{2} \left(1 - \frac{1}{n^4} \right)  =
	\frac{1}{2} \left( 1 + \frac{1}{n^4} \right) = \frac{n^4 + 1}{2n^4}\\
	\text{Moving to the right } \;\; q &= 1 - p = \frac{n^4 - 1}{2n^4}
\end{align*}

Now, let $g(x) = nx - x^2$. We will now show that there exists a constant $c$ such that 
\begin{align*}
 E[g(X_{t+1}) | X_t = x] \leq g(x) - c \;\; for \; all \;x \in S \;with \;g(x) > 0
\end{align*}
So that we can later apply Theorem 9.12.

We can find an upper bound for the expectation in the following way:
\begin{align*}
	E[g(X_{t+1}) | X_t = x]  &= p \cdot E[g(X_{t+1}) | X_{t+1} = x - 1] + q \cdot E[g(X_{t+1}) | X_{t+1} = x+1] 
	\\	&= p \cdot g(x-1) + q \cdot g(x+1) 
	\\  &= \frac{n^4 + 1}{2n^4} (nx - n - x^2 - 1 + 2x) + \frac{n^4 - 1}{2n^4} \cdot (nx + n - x^2 - 1 - 2x)
	\\  &= nx - x^2 - 1 - \frac{1}{n^3} + \frac{2x}{n^4}
	\\  &= g(x)  - (1 - \frac{2x}{n^4} + \frac{1}{n^3})
\end{align*}

Now, we are interesting in bounding $(1 - \frac{2x}{n^4} + \frac{1}{n^3})$ with a constant $c$ for every possible value of $x$. Since $(1 - \frac{2x}{n^4} + \frac{1}{n^3})$ is maximised for $x = n$, we have:
\begin{align*}
	1 - \frac{2n}{n^4} + \frac{1}{n^3} = 1 - \frac{1}{n^3} 
\end{align*}
So, we can use $1 - \frac{1}{n^3}$ as our constant $c$.

To use Theorem 9.12, we use as starting state $x_0$ the state that maximises $g(x)$, and that is when $x = \frac{n}{2}$.
Concluding, we apply Theorem 9.12 to obtain:
\begin{align*}
	E[T] \leq \frac{g(x_0)}{c} = \frac{\frac{n^2}{2} - \frac{n^2}{4}}{1 - \frac{1}{n^3}} 
	= \frac{n^2}{4} \left(\frac{n^3}{n^3-1}\right)   = O(n^2)
\end{align*}


\section*{Solution to Exercise 2}
\textbf{Part a} \\

Let $P$ be a set of $3k(n)$ vertices $v_0, v_1, \ldots,  v_{3k(n) - 1}$ in $G_{n,p}$.
We can calculate that 
\begin{align*}
	Pr [ P \text{ contains a valid Toblerone }] = p^{3k(n)} p^{3(k(n) - 1)} (3k)!
\end{align*}
This is because there must be $3k(n)$ edges that form $k(n)$ triangles and $3(k(n) - 1)$ edges that 
form the path between the triangles; the $(3k)!$ term is there because we do not care about the ordering of the triangles or the ordering of the points inside a single triangle. Now, we know that on a given graph with $n$ vertices
there are ${n \choose 3k}$ differents $P$ sets. Let's define the random variable $X_0$ that assumes value
1 if set $P_0$ contains a valid Toblerone, and 0 otherwise. Let's now define 
$X$ as $X = X_1 + X_2 + \ldots + X_{n \choose 3k(n)}$. 
For the linearity of expectation, we can calculate $E[X]$ in the following way:
\begin{align*}
	E[X] = \sum_{i=1}^{n \choose 3k(n)} E[X_i]
	= {n \choose 3k(n)} p^{3k(n)}p^{3(k(n) - 1)}(3k(n)!) 
	\\  = \frac{n!}{(n - 3k(n))!(3k(n))!} (3k(n))!p^{6k(n) - 3} 
	= \frac{n!}{(n - 3k(n))!} p^{6k(n) - 3}
\end{align*}

We can now apply Stirling to both the numerator and denominator to get:

\begin{align*}
	\frac{n!}{(n - 3k(n))!} \cdot p^{6k(n) - 3} = \frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}
	{\Theta(n - 3k(n)) (\frac{n - 3k(n)}{e})^{n - 3k(n)}}p^{6k(n) - 3}
\end{align*}

Since $k(n) = (\frac{1}{3} - \epsilon) n$, we have:

\begin{align*}
	\frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}
	{\Theta(\sqrt{n - 3k(n)}) (\frac{n - 3k(n)}{e})^{n - 3k(n)}} p^{6k(n) - 3}
	= \frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}{\Theta(\sqrt{3\epsilon n})(\frac{3\epsilon n}{e})^{3\epsilon n}} p^{2n - 6\epsilon n - 3}
\end{align*}
Then:
\begin{align*}
	\frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}{\Theta(\sqrt{3\epsilon n})(\frac{3\epsilon n}{e})^{3\epsilon n}} p^{2n - 6\epsilon n - 3}
	= \frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}{\Theta(\sqrt{n})(\frac{3\epsilon n}{e})^{3\epsilon n}} p^{2n - 6\epsilon n - 3}
\end{align*}

Now, we have that 
\begin{align*}
	p^{2n - 6\epsilon n - 3} = o( n^{-\frac{1}{2}(2n - 6 \epsilon n - 3)}) 
	= o (n ^{-\Theta(n)})
\end{align*}
And that
\begin{align*}
\frac{\Theta(\sqrt{n}){(\frac{n}{e})^n}}{\Theta(\sqrt{n})(\frac{3\epsilon n}{e})^{3\epsilon n}}  
	= O(1) \left(\frac{n}{e}\right)^n  \left(\frac{e}{3\epsilon n}\right)^{3\epsilon n}
	= O(n^n)
\end{align*}
In conclusion, we have that
$E[X] = O(n^n) o(n^{-\Theta(n)}) = o(1)$.

This means that the expected number of Toblerone subgraphs in $G_{n,p}$ is 0 whp.




\section*{Solution to Exercise 3}
\textbf{Part a} \\
\emph{Explanation:} We will show, using Lemma 10.9 from the lecture notes, that the stationary distribution of the  Markov chain $M_t$ defined in the exercise is the uniform distribution $\pi = \frac{1}{n!}$
\vspace{1em}

The Markov chain $(M_t)_{t\in\mathbb{N}}$ defined in the exercise has a state space $S$ corresponding of every possible shuffling of the $n$ cards, thus $|S| = n!$.
We know that, given a pair $i$ in the deck, the two cards of the pair will end up in position $i$ or $i + \frac{n}{2}$ depending on the outcome of a throw of a fair coin. Since for every pair of the deck we can have two different outcomes, in total we can have $2^{\frac{n}{2}}$ possible outcomes, each with probability $2^{-\frac{2}{n}}$. Furthermore, every outcome is different, because every position in the deck can be reached only by a single pair. 
This means that in our Markov chain, every state has $2^{\frac{n}{2}}$ outgoing edges.

Let's now count the number of incoming edges for each state. Consider an
arbitrary state $X_t$ of the Markov chain. Now, given $i < \frac{n}{2}$,
consider the two cards at positions $i, i + \frac{n}{2}$. Those two cards can
only come from the $i-th$ pair of the deck of the previous state $X_{t-1}$ of
the Markov chain.
This means that, for every $i \in \{ 1, 2, \dots, \frac{n}{2}\}$, the tuple of cards at position $i, i + \frac{n}{2}$ can only come from two different states of the Markov chain. Since there are $\frac{n}{2}$ possible tuples, then there are $2^\frac{n}{2}$ incoming edges for each state.

In conclusion, we know that each state has $2^\frac{n}{2}$ incoming edges and
$2^\frac{n}{2}$ outgoing edges, so this means that the every column and every
row of the transition matrix of the Markov chain will have the sum equal to 1,
proving that it is indeed doubly stochastic. Since we can also assume that the
Markov chain is ergodic, the conditions of Lemma 10.9 are satisfied. So, the
stationary distribution is the uniform distribution $\pi =
\frac{1}{|S|} = \frac{1}{n!}$.


\vspace{1em}
\noindent 
\textbf{Part b} \\



Before moving on to the proof, we first have to introduce a way of representing the position of a card in the deck.
Since there are $n = 2^k$ cards in total, we can represent the position of a card with a bitstring of $k$ bits (where the most significant bit is the leftmost one).
We can also represent the movement of a card in the following way: the bitstring is shifted to the right, dropping the least significant bit, and inserting to the left a bit depending on the outcome of the throw of the fair coin (a $1$ will be added if the card is going to be moved to the second half, a $0$ otherwise).

Now, let's consider the two independent copies of the Markov chain $X(t)$ and $Y(t)$. We need to calculate the expected number of steps until two cards with the same label will end up in the same position. 
To do that, we will define a new Markov chain $Z(t')$ in which the states represent the lenght of the common prefix
of the bitstrings encoding the position of the two cards. For example, if the first card is represented
by the bitstring $101110$ and the second one by $101000$, they will be in state $3$. State $k$ is reached when the two cards have exactly the same bitstrings and thus are in the same position in the deck. Every state $i$ with $i < k$ will have $\frac{1}{2}$ of probability of ending up in state $i+1$ and $\frac{1}{2}$ probability of ending up in state $0$ ; instead, state $k$ is absorbing. We are interested in the expected number of steps to end up in state $k$.

Let's define $d_i$ as the expected number of steps to go from state $i$ to state $k$. We know that $d_k = 0$, and that $d_i = \frac{1}{2} d_{i+1} + \frac{1}{2} d_0 + 1$. We can easily prove an upper bound on $d_i$ without solving the recurrence relation in the following way:
\begin{align*}
 d_0 = \frac{1}{2} d_0 + \frac{1}{2} d_1 + 1\\
 d_0 (1 - \frac{1}{2}) = \frac{1}{2} d_1 + 2\\
\end{align*}
\begin{align*}
	d_0 &= d_1 + 1 \\
		&= 4 + 2 + d_2\\
		&= 8 + 4 + 2 + d_3\\
		& \ldots\\
		&= \sum_{i=1}^k 2^i = 2^{k+1} - 1 \leq 2 \cdot 2^k = 2n
\end{align*}
This shows that, for every $i$, the expected number of steps to go from state $i$ to state $k$ is upper bounded by $2n$.
This concludes the proof that $E[T] \leq Cn$ where $T$ is the smallest positive integer $t$ such that two cards with the same label end up in the same position in the two Markov chains $X$ and $Y$ after $t$ steps, and where $C = 2$.


\vspace{1em}
\noindent 
\textbf{Part c} \\
We will reuse the Markov chain $Z(t)$ that helped us calculate $E[T]$ in the
previous exercise, where $T$ is the smallest positive integer $t$ such that the
the cards ends up in the same position after $t$ steps. Recalling how it worked,
we represented the position of the cards in the deck as a $k$ long bitstring, 
and on each step of the chain the cards could be getting closer to state $k$
by 1 with probability $\frac{1}{2}$ or go all the way back to state $0$ with 
probability $\frac{1}{2}$.

This notion will be useful to us in this part too, in fact we can introduce a
Bernoulli variable $\Omega_i$ = Bernoulli($\frac{1}{2}$) for each step of the
chain, having a value of 1 if after a single step the distance decreased by 1
(analogously, if it went from state $i$ to state $i+1$ in $Z(t)$), or a
value of 0 if the distance reset to $k$ (analogously, if it went from state
$i$ to state $0$). With this notation, if $k$ consecutive Bernoulli
variables $\Omega_i, \ldots \Omega_{i+k-1}$ assume value of 1, this means that
we're in state $k$ of $Z(t)$ and the cards are in the same position in the
two decks.

We will now use Janson's inequality.
Let $A_i = \{i, i+1, \ldots , i+k-1\}$ the set of $k$ consecutive integers starting from $i$.
Now let $X_i$ be the indicator variable for the event that all coordinates from $A_i$ are
equal to 1 (that is, $X_i$ = 1 only if $\Omega_j = 1 \;\; \forall j \in A_i$). Furthermore,
let $X = \sum_{i=1}^m X_i$, where we can set $m = t(n) - k$, so that in this way $X$ will 
be 0 only if $T > t(n)$.

Then, we have
\begin{align*}
 \lambda := E[X] =  \sum_{i=1}^m X_i = m2^{-k} \;\; \text{for the linearity of expectation}\\
\end{align*}
\begin{align*}
	\Delta := \sum_{\substack{i\neq j \\ A_i \cap A_j \neq 0}} Pr[X_i = 1 \wedge X_j = 1] =
	\sum_{\substack{i\neq j \\ A_i \cap A_j \neq 0}} Pr[X_i = 1|X_j = 1]Pr[X_j = 1] = \\
	\sum_{i=1}^m 2^{-k} \sum_{y=1}^{k - 1} 2^{-(k-y)} = m2^{-2k}\sum_{y=1}^{k-1}2^y = 2^{-2k}m2^k =
	m2^{-k}
	%\sum_{\substack{i\neq j \\ A_i \cap A_j \neq 0}} 2^{-(k-y)-k} \;\; \text{where} \;\; y = j - i
\end{align*}

We can now apply Janson and obtain the following:
\begin{align*}
	Pr[ T > t(n)] = Pr[X = 0] \leq e ^ {-\min\{\lambda, \frac{\lambda^2}{\Delta}\}/4} = 
	e^{-\frac{\lambda^2}{4\Delta}} = e ^ {- \frac{m^2 2^{-2k}}{4m 2^{-k}}} = e^{- \frac{m}{4n}}
\end{align*}
Since $m = t(n) - k$, we have:
\begin{align*}
	Pr[ T > t(n)] \leq e ^ {-\frac{t(n) - k}{4n}} =  e^{- \Omega(t(n)/n)}
\end{align*}


\vspace{1em}
\noindent 
\textbf{Part d} \\
\emph{Explanation:} We will use Lemma 10.17 from the lecture notes to show that $(M_t)_{t \in \mathbb{N}}$ is rapidly mixing.

Given $(X_t)_{t\in\mathbb{N}} $ and $(Y_t)_{t\in\mathbb{N}} $ two Markov chains that follow the definition
of the shuffling in the exercise, we build a coupling $Z_t = (X_t, Y_t)$. We build the coupling in 
such a way that $X_i(t) = Y_i(t)$ implies that $X_i(t+1) = Y_i(t+1)$. This can be done in the following
way: the Markov chain $X(t)$ runs normally - meaning the result of the throw of the coin for each
pair is chosen uniformly at random; insteand, for $Y(t)$, if a card of a given label occupies the
same position in both decks $X$ and $Y$, then the result of the throw of the coin for the pair
relative to that card will be the same as for $X(t)$; all other pairs that don't contain a card
that has the same position in both decks will have a fair coin thrown. 
This ensures that once a card occupies the same position in both decks, it will do so for every 
step starting from that point. Since both $X(t)$ and $Y(t)$ each independently behave like
the original Markov chain $(M_t)_{t\in\mathbb{N}}$, the Definition 10.15 of coupling is satisfied and
we can use Lemma 10.17. 

But first, we need to find a $t_0$ such that 
\begin{align*}
	Pr [ X_{t_0} \neq Y_{t_0} | X_0 = x, Y_0 = y] \leq \frac{1}{2}
\end{align*}
is satisfied for all $x,y\in S$.

We will now show that $t_0 = n^2$ satisfies this condition. 
Let's define event $A_i$ as  "the position of the card labeled $i$ is different in the two decks after $t_0$ steps", and event $B$ as "there is at least one card with different position in the decks after $t_0$ steps".

We can now see that, for the union buond, $Pr[B] \leq \sum_{i=1}^n Pr[A_i]$.
Since $Pr [ X_{t_0} \neq Y_{t_0} | X_0 = x, Y_0 = y] = Pr[B]$, and since $t_0 = n^2 = \omega(n)$, we can use the result of the previous part ($Pr[ T > t(n)] \leq e^{- \Omega(t(n)/n)}$) and we have that:
\begin{align*}
	Pr[B] \leq \sum_{i=1}^n Pr[A_i] 
	\leq ne^{-\Omega(\frac{n^2}{n})} = ne^{-\Omega(n)} \leq \frac{1}{2}
\end{align*}

So, all conditions of Lemma 10.17 are satisfied and we can conclude by applying it:
\begin{align*}
	\tau_{TV(\epsilon)} \leq \log(\epsilon ^ {-1}) \cdot t_0 = \log(\epsilon^{-1}) n^2 = O(poly(n, \log \epsilon))
\end{align*}


\end{document}
