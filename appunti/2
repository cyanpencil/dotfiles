\documentclass[a4paper,10pt]{article} %default
\usepackage{geometry}
\usepackage{fontspec}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{listings}
%\usepackage{tikz}
%\usetikzlibrary{arrows.meta}
%\usetikzlibrary{graphs,graphdrawing}
%\usegdlibrary{force}
\geometry{top = 3mm, lmargin=5mm, rmargin=5mm, bottom=3mm}
\pagestyle{empty}
\setlength{\parindent}{0pt}

\lstset{
    basicstyle=\ttfamily,
    numberstyle=\ttfamily,
    numbers=left,
    backgroundcolor=\color[rgb]{0.9,0.9,0.9},
    columns=fullflexible,
    keepspaces=true,
    frame=lr,
    framesep=8pt,
    framerule=0pt,
    xleftmargin=20pt,
    xrightmargin=30pt,
    mathescape
}

\newcommand{\specialcell}[2][c]{%
    \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
\newcommand{\dimo}[1]{%
    \smallbreak \par \hfill\begin{minipage}{0.92\linewidth}{ \scriptsize {\textbf{\em{Dim.}}} {#1} }\end{minipage} \smallskip \par}
\newcommand{\mtheorem}[1]{%
    {\hspace*{-10pt} \textsc {#1}}}
\newcommand{\malgorithm}[1]{%
    {\bigbreak \par \hspace*{4pt} \underline{\textbf {#1}}}}
\newcommand{\msection}[1]{%
    {\newpage\bigbreak \bigbreak \par \hfil \huge \textsc {#1}}\par}
\renewcommand{\b}[1]{%
    {\textbf{#1}}}
\renewcommand{\t}[1]{%
    {\texttt{#1}}}
\newcommand{\mdef}[1]{%
    {\smallbreak\par\begin{tabular}{ll} \textbf{Def.$\;\;$} & \begin{minipage}[t]{0.80\columnwidth}\normalsize  {#1}\end{minipage}\tabularnewline \end{tabular}}\smallskip\par}
\newcommand{\mcomment}[1]{%
    {\hfill \scriptsize {#1}}}
\newcommand{\mprop}[1]{%
    {\smallbreak\par\begin{tabular}{ll} \textbf{Prop.} & \begin{minipage}[t]{0.8\columnwidth}\emph  {#1}\end{minipage}\tabularnewline \end{tabular}}\smallskip\par}

\begin{document}

\scriptsize

I dati da mandare vengono raggruppati in \b{pacchetti}. Quelli che si occupano di trasmetterli si chiamano \b{packet switches}. La maggior parte di essi usano la \b{store and forward} transmissio, cioè devono aspettare di ricever l'intero pacchetto prima di processarlo. Pericò i pacchetti in uscita si potrebero impilare nell'\b{output buffer} (o queue). Se l'output buffer è pieno, avviene un \b{packet loss}. Inoltre, ogni router ha una \b{forwarding table} per sapere dove mandare un pacchetto in base al suo indirizzo nell'header. 
Opposto al packet switching è il \b{circuit switching}, dove come nei telefoni prima viene stabilita una connessione, e poi vengono liberamente mandati i dati. Per stabilire più circuiti su uno stesso link si può usare la \b{FDM} (frequency division multiplexing), in cui a un circuito viene attribuita solo una frazione della bandwidth del link(proprio come nella radio), oppure la \b{TDM} (time division multiplexing), in cui il tempo in un link viene diviso in segmenti, e ogni circuito può usare tutta la bandwidth ma solo nei suoi segmenti. I due problemi principali del circuit switching sono i \b{periodi di silenzio} e le grandi difficoltà di implementazione. Il vantaggio è che permettono scambio di dati con minore ritardo.

Ogni nodo, prima di ricevere e mandare pacchetti, ha vari tipi di delay: \b{processing delay}: il tempo che ci vuole ad analizzare l'header di un pacchetto, a controllare eventuali errori. Di solito intorno al microsecondo; \b{Queueing delay}: dipende da quanti pacchetti devono essere trasmessi. Varia da 0 a millisedondi; \b{Transimission delay}: Il tempo che ci vuole a trasmettere un pacchetto attraverso il link. Se L è il numero di bit da mandare, R è la capacità del link, allora li tempo è L/R. Di solito varia da qualche microsecondo a qualche millisecondo; \b{Propagation delay}: è il tempo che ci mette il pacchetto ad arrivare a destinazione. Se d è la distanza e s la velocità del link, il tempo è d/s. s dipende dal materiale del link, ma spesso è vicino alla velocità della luce.

\b{Traffic Intensity}: (L*a)/R, dove L è il numero di bit di ogni pacchetto, a è la media di quanti pacchetti al secondo arrivano, R è la velocità di trasmissione di un pacchetto. Se la traffic intensity è maggiore di 1, allora la queue d'attesa diventerà infinita. Tuttavia è da notare che è una funzione esponenziale: più è vicina a 1, più la lunghezza della queue cresce esponenzialmente. Quindi in ogni network dovrebbe sempre essre strettamente minore di 1, ma più bassa è meglio è.
\b{Traceroute}: un semplice programma che manda N pacchetti speciali a una destinazione verso cui deve fare N - 1 \b{hop}. Ogni router che riceve il suo pacchetto speciale rimanda indietro un messaggio di successo (anche la destinazione). Traceroute misura il tempo che ci mette ogni messaggio a tornare. Ripete l'esperimento 3 volte. 
\b{Throughput}: velocità a cui un file viene mandato da sorgente a destinazione. Se non c'è traffico, è uguale alla velocità di trasmissione del link più lento attraversato. 

\b{Protocol Stack}: insieme dei protocolli di ogni layer. Il protocol stack di internet è composto da 5 layer: \b{Application}: un protocollo applicativo è distribuito su vari end systems, con un'applicazione su ogni end system. I pacchetti scambiati nel protocollo applicativo si chiamano messaggi. \b{Transport}: i protocolli di trasporto si occupano di trasportare messaggi fra due applicazioni. I pacchetti scambiati si chiamano segmenti. \b{Network}: IP protocol  e vari altri routing protocol. Si occumano di prendere i segmenti e farli arrivare a destinazione. I pacchetti scambiati prendono il nome di datagram. \b{Link}: Si occupa di far muovere i datagram tra i vari nodi della rete. Alcuni esempi sono Wi-Fi e Ethernet. Può essere che un datagram utilizza diversi link protocol nel suo viaggio. I pacchetti scambiati sono i frame. \b{Physical}: dipendono dal mezzo usato per trasmettere. Si occupano di mandare bit per bit i frame attraverso un link. 

\b{Encapsulation}: ogni layer prende il pacchetto del layer superiore e o incapsula aggiungendo informazioni. Il pacchetto incapsulato si chiama \b{payload}, mentre le informazioni aggiunte sono l'header. Gli host implementano tutti e 5 i layer. I router solo i primi 3. I link switches solo i primi 2. 
Nel tardo 1970, nel modello OSI, c'erano altri due layer, presentation e session layer; il primo si occupava di descrivere i dati trasferiti(compressione...), mentre il secondo si occupava di delimitare e sincronizzare lo scambio di dati. Oggi questi servizi sono implementabili a livello applicativo.

\b{Socket}: interfaccia software che permette ai processi di mandare e ricevere messaggi. In pratica è l'interfaccia fra l'pplication layer e il transport layer. Il programmatore ha il totale controllo della parte applicativa, ma della parte di trasporto può decidere solo il tipo di protocollo e alcuni parametri.
Le architetture applicative sono distinte in due grosse categorie: \b{Client-server architecture}: ci sta un host, il server, sempre attivo, che soddisfa le richieste di vari host temporanei, i client. Se un solo server non basta, si possono impiegare vari datacenter. Due client non possono comunicare fra loro senza passare attraverso il server. Il server ha IP fisso. \b{Peer-to-peer architecture}: l'applicazione usa comunicazione diretta tra coppie di host temporanei chiamati peer. Nesun server. I vantaggi principali del P2P sono la self-scalability, la failure prevention e i bassissimi costi. Tuttavia nel futuro ci saranno vari problemi: non sono ISP-friendly (richiedono anche molto upstream), non è detto che siano sicuri, e non è detto che gli utenti vogliano condividere.

\b{UDP} sta per User Datagram Protocol. Fornisce i servizi di multiplexing/demultiplexing e di integrity checking.
\b{Demultiplexing}: il transport layer si occupa, quando l'host riceve un messaggio, di inviarlo al processo applicativo giusto dell'host. E' in grado di farlo grazie alla presenza dei \b{socket}. \b{Multiplexing}: il transport layer raccoglie tutti i messaggi uscenti dai vari socket dell'host e li raggruppa in vari grandi segment.
Un messaggio UDP è quindi formato dalla source port number e dalla destination port number (in caso ci sia bisogno di una risposta), e da altri due valori. Dopo viene passato al network layer, che ci scrive source IP e destination IP.
Importante: un socket UDP viene identificato solamente dalla coppia destination IP e destination port number.

UDP è il protocollo di trasporto più semplice che esista. Viene usato spesso da \b{DNS}. Non ci sta nessun tipo di handshake. Se dopo un certo tempo non ricevo nessuna risposta, allora viene mandata un altra richiesta.

Vantaggi principali di UDP sopra a TCP:
1) Controllo sui dati inviati: UDP aggiunge solo un piccolo header ai dati e passa tutto al network layer. Non ci sta nessun tipo di congestion control, quindi nessun eventuale attesa. Inoltre, non è detto che debba avere come condizione l'integrità dei dati.
2) Nessuna connessione: TCP usa un three-way handshake prima di inviare dati, che introduce notevoli ritardi.
3) Nessuno stato di connessione: TCP salva molti dati sullo stato di una connessione, UDP invece zero.
4) Piccolo overhead dei pacchetti: un pacchetto TCP ha 20 byte di overhead, mentre un UDP solo 8 byte.

L'header UDP è composto da 4 campi, ognuno largo 2 byte. Oltre a source e destination port, ci stanno length e checksum. La \b{checksum} è calcolata come il complemento a uno della somma di tutte le parole a 16 bit del messaggio. Il ricevente, se somma la checksum a tutte le parole del messaggio, dovrebbe ottenere tutti uni. L'error checking è stato introdotto poichè non è detto che tutti i link implementino error checking fra di loro, e ci potrebbero anche essere altri generi di errori. UDP non offre nessuna maniera di correzione dell'errore.


\b{HyperText Transfer Protocol}: Una \b{Web Page} (documento) è formata da \b{oggetti}. I client sono i \b{Web browsers}, invece i server sono i \b{Web server}. HTTP usa TCP per mandare messaggi, quindi non si deve preoccupare di eventuali errori. Siccome HTTP manda i file richiesti senza salvarsi nulla del client, prende il nome di \b{stateless protocol}. 
\b{Round Trip Time}: tempo che passa da quando il client manda un pacchetto al server a quando una risposta torna al client.
HTTP può essere configurato per usare la \b{non persistent connection}, cioè creo una nuova connessione TCP per ogni file che devo mandare. Potrebbe essere lento poichè una web page è formata da vari file, ogni file ci mette 2 RTT + tempo di trasmissione del file. Tuttavia, di default usa la \b{persistent connection}, cioè dopo aver risposto a ua richiesta, il server non chiude la connnessione, ma può rispondere ad altre richieste. Non solo. Le richieste possono esere fatte tutte di seguito, senza aspettare le risposte (\b{pipelining}).
Una request in HTTP è composta da una o più linee codificate in ASCII, ognuna seguita da un carriage return e da un line feed. La prima linea è la \b{request line}, formata da 3 campi: il campo del metodo("GET, POST, HEAD, PUT, DELETE"), il campo URL, e il campo della versione ("HTTP/1.1"). Le linee successive sono le \b{header lines} ("host, connection, User-Agent, Accept-Language") per fornire varie informazioni. Poi segue una riga vuota, e poi ci sta l'entity body, che contiene informazioni nel caso del POST.
Una risposta invece è formata da una \b{status line}, composta da 3 campi: versione HTTP, codice di status e status message. Dopo ci sono le header lines, con campi come ("Date, Server, Last-Modified, Content-length, Content-Type"). Alcuni status code: 200 OK (Tutto a posto); 301 MOVED PERMANENTLY (la risorsa si trova nell'URL specificato dal campo "Location"); 400 BAD REQUEST; 404 NOT FOUND; 505 HTTP VERSION NOT SUPPORTED.
I \b{cookie} invece non sono altro che del testo che il browser salva accanto al nome dell'host qunado riceve "Set-cookie:" nell'header della risposta. Vengono rimandati quando ci si riconnette a quell'host. 
Le \b{Web Cache} sono dei proxy server installate dalle ISP per tentare di ridurre il traffico (e ravvelocizzarlo motlo, se si fanno molte cache hit). Per capire se il file della cache è vecchio, la cache può usare il \b{conditional GET}, con il campo "If-modified-since". Se la copia non è vecchia, il server risponderà con 304 NOT MODIFIED.


\b{File Transfer Protocol}: molto simile a HTTP, anche lui usa TCP. Una grande differenza è che usa una doppia connessione TCP, una control connnection e una data connnection. I file vengono inviati nella data connnection, mentre i comandi nella control. La data connection viene usata per inviare un solo file, poi viene chiusa. Nel caso, ne viene aperta un altra. Un'altra differenza è che il server mantiene uno \b{state} per ogni client, in cui si salva la cartella su cui sta, i suoi permessi etc.
Come in HTTP, gli header FTP sono formattati in ASCII. I comandi più comuni sono: "USER, PASS, LIST, RETR, STOR". A ogni comando, segue una risposta. Niente pipeline (?).


\b{SMTP}: si occupa di trasferire le mail dal server del sender al server del ricevente. E' molto vecchio, ha alcune cose strande, come per esempio il body della mail deve essere per forza codificato in 7-bit ASCII, e quindi non posso mandare audio, video etc.
SMTP trasferisce le mail tra due server senza server intermediari, direttamente dal mittente al destinatario. Se il server del destinatario è irraggiungibile, allora SMTP riprova dopo un tot di tempo. Ovviamente, usa TCP. La porta usata è la porta 25.
Una volta stabilita la connessione, ci sta un po' di handshaking, e dopo invia il messaggio. I comandi che si scambiano tra server sono del genere ("HELO, MAIL FROM, RCPT TO, DATA, QUIT"). Una linea che consiste di un solo punto corrisponde alla fine della mail. Il server risponde a ogni comando.  SMTP usa persistent connection, e quindi se deve inviare varie mail lo fa attraverso una singola connessione TCP.
Le differenze principali con HTTP: HTTP è principalmente un \b{pull protocol}, usato per scaricare risorse dai server. Invece, SMTP è principalmente un \b{push protocol}, usato per mandare dati ai server. Inoltre, HTTP non forza il body del messaggio a essere codificato in 7-bit ASCII. Una terza differenza sta nel fatto che ogni oggetto di una pagina Web viene gestito con un messaggio HTTP diverso (uno per il testo, uno per le immaggini, etc.)
\b{Mail formats}: Tutte le mail, all'inizio del messaggio, hanno alcune header lines, come ("To:, From:, Subject:"), e dopo una riga bianca, contengono il vero e proprio messaggio. Queste linee sono diverse dai comandi SMTP usati per l'handshaking.
Perchè usiamo dei mail server gestiti dagli ISP? Poichè se avessimo un mail server sul nostro computer, significherebbe che il nostro computer dovrebbe rimanere sempre acceso, e soprattutto, connesso a Internet. Quindi serve anche un protocollo per scaricare mail da un mail server.
\b{POP3}: inizia quando il client apre una connessione TCP sul mail server sulla porta 110. Tre fasi: autorizzazione (invio di user e password), transaction (scarico messaggi, ed eventualmente eliminazione dal server e altre azioni varie), e update (chiude la connessione, elimina le mail marcate da eliminare dall'utente). A ogni comando segue una risposta, che può essere ("+OK, -ERR"). I comandi possono essere ("user, pass, list, retr, dele") etc.  POP3 tiene uno \b{state} per ogni connessione, cioè quali messaggi sono stati marcati per l'eliminazione. Si può configurare che il server cancella un messaggio dopo che è stato scaricato ("Download-and-delete"), oppure no ("Download-and-keep")
\b{IMAP}: un server IMAP associa a ogni mail una cartella, all'inizio tutte nella \b{inbox}, e poi possono essere spostate dall'utente. Ha anche funzioni di ricerca. Al contrario di POP3, IMAP si tiene lo \b{state} anche quando la connessione viene chiusa, come le cartelle e il loro nome. Un altra feature è che un user agente può richiedere di scaricare solo una parte della mail (come l'header) nel caso di connessioni lente.
\b{Web-based Email}: il più diffuso oggi, sfrutta il protocollo HTTP.


\b{Domain Name System}: è sia un sistema distribuito di database, sia il nome di un application layer protocol. DNS usa UDP e va sulla porta 53, serve a tradurre hostname in indirizzi IP, ed è costantemente utilizzato da altri application layer protocol come HTTP e FTP. Inoltre, offre anche i servizi di \emph{host aliasing} (vari nomi per un singolo host), \emph{mail server aliasing}, \emph{load distribution} (quando un'azienda ha vari server con vari IP, i server DNS restituiscono gli IP a rotazione per distribuire il carico sui vari server).
\b{Root DNS server}: ce ne sono solo 13, la maggior parte negli Stati Uniti.
\b{Top-level Domain server}: sono responsabili per i domain principali come .com, .net, .edu, .it etc.
\b{Authoritative DNS Server}: un server DNS specializzato solo per gli host di una o più compagnie.
Dopodichè, ci sono i \b{local DNS server}, che non sono per forza nella gerarchia DNS: ogni ISP ne ha uno, e servono per collegarsi ai DNS più grossi.
Le query a un server DNS possono essere \b{recursive} (il local DNS interroga il root, che interroga il top-level domain server, che interroga l'authoritative) oppure \b{iterative} (il local DNS interroga il root, poi, dopo aver ricevuto una risposta, interroga il top-level domain etc..)
\b{DNS Caching}: Spesso vengono fatte le stesse identiche query ai local DNS, che quindi si salvano i record. A volte si salvano anche gli IP dei server top-level domain, per evitare di passare attraverso il root.
I server DNS si salvano i dati in forma di \b{Resource Records (RR)}, che hanno la forma (Name, Value, Type, TTL). TTL non è altro che il tempo di vita di un record (in una DNS cache, dopo quanto tempo deve essere rimosso). Se Type=A, allora Name è l'hostname e Value è l'indirizzo IP. Se Type=NS, allora Name è un domain ("foo.com") e Value è l'hostname dell'Authoritative DNS che conosce il suo IP.  Se Type=CNAME, allora Value è il vero hostname di un alias di nome Name. Se Type=MX, allora Value è il vero hostname di un server mail che ha come alias Name.
\b{DNS Messages}: sono formati da un header, di 12 byte, con la query, varie flags, e la lunghezza dei vari pezzi del body. Nel body, ci sono le questions (hostname e Type), le answers (Type, Value, TTL), le authority, e le additional informations.


\b{BitTorrent}: L'insieme di peer coinvolto nella distribuzione di un particolare file prende il nome di torrent. I peer in un torrent stanno condividendo chunk di quel file di dimensioni prefissate (di solito 256 KB). Mentre un peer scarica chunk, allo stesso tempo condivide quelli che ha. Ogni torrent ha un'infrastruttura chiamata \b{tracker}. Quando un peer si unisce a un torrent, si registra sul tracker, e periodicamente informa al tracker che ci sta ancora. In questo modo, un tracker ha la lista di tutti i peer del torrent. Appena un peer si unisce, il tracker manda una lista di una parte degli IP di altri peer (di solito 50). Il peer prova a formare connessioni TCP con questi altri 50. Alcuni di questi se ne andranno, ma altri si connetteranno al nuovo peer. Periodicamente, un peer chiede ai suoi \b{neighboring peers} la lista dei chunk che posseggono. Dopodichè, inizia a fare richieste ai suoi vicini, partendo dai chunk più rari. Inoltre, deve iniziare a condividere i chunk che ha. Una richiesta di un chunk in BitTorrent viene soddisfatta se il peer richiedente è \b{unchocked}, cioè è uno di quei peer che ti stanno inviando più dati. Inoltre, ogni peer, ogni 30 secondi, sceglie a caso un altro peer e soddisfa le sue richieste. Questo peer preso a caso diventa \b{optimistically unchocked}. Se questo nuovo peer ha dei file che piacciono al peer originario, questi due potrebbero iniziare a scambiarsi dati velocemente e diventare unchocked. Questo peer preso a caso diventa \b{optimistically unchocked}. Se questo nuovo peer ha dei file che piacciono al peer originario, questi due potrebbero iniziare a scambiarsi dati velocemente e diventare unchocked. Tuttavia, Bittorrent non è un puro P2P, in quanto si basa sul server tracker.


\b{Stop\&Wait}: Un protocollo Stop\&Wait ha un sistema di reliable data transfer basato sull'error checking. In pratica, mando un messaggio e aspetto una risposta di tipo \t{ACK} o \t{NAK}. In base a quello, mando un altro messaggio o ripeto il precedente. Il ricevente invece ha un singolo stato, riceve un pacchetto e manda conferma negativa o positiva. Inoltre, ci deve essere anche un sistema decente di controllo di errori. Tuttavia dobbiamo aggiungere un campo \t{sequence number}, da 1 bit, per indicare se il pacchetto mandato è uno nuovo oppure è una retrasmissione. Questo risolve il problema di capire che fare se arriva un pacchetto di conferma \t{ACK} corrotto: semplicemente rimando il pacchetto di nuovo.

\b{Alternating bit protocol}: uguale allo stop and wait, solo che mittente, dopo un certo tempo che non ha avuto nessuna risposta, rimanda il pacchetto. 

Tuttavia, il metodo stop and wait è molto lento, poichè gli host stanno moltissimo tempo ad aspettare senza fare nulla. La soluzione è inviare pacchetti anche se non si è ricevuto nessuno acknowledgment.

\b{Go-Back-N}: è uno sliding window protocol, cioè ammette l'attesa di al massimo $N$ acknowledgments. In pratica posso la mia window di pacchetti che non so se sono arrivati bene oppure no è lunga $N$. Questo limite esiste per il congestion e il flow control. E' necessario un field con il sequence number del pacchetto. Il mittente, ogni volta che manda un pacchetto, deve controllare se la window è piena. Inoltre, si deve tenere conto di fino a quale \t{ACK} ha ricevuto (\b{cumulative acknowledgment}, cioè se ricevo un \t{ACK 30} vuol dire che il destinatario a ricevuto bene tutti i pacchetti fino al 30). Inoltre, deve avere timer che se scade, rimanda tutti i pacchetti di cui non ha avuto \t{ACK}. Appena ne ricevo uno, resetto il timer. Il ricevente, se riceve pacchetti in un ordine sbagliato, li butta. Non si salva pacchetti che avrebbe dovuto ricevere dopo, perchè tanto se ne manca qualcuno prima, vuol dire che anche tutti quelli dopo verranno rimandati

Tuttavia, Go-Back-N ha qualche difetto: se la window size e grossa, e invio molti pacchetti nella pipeline, mi basta che ci sia un solo errore e li devo rimandare tutti quanti.

\b{Selective Repeat}: tecnica uguale a Go-Back-N, solo che rimanda solo i pacchetti (potenzialmente) coinvolti nell'errore. Una grossa differenza è che il receiver deve fare l'acknowledgment di ogni singolo pacchetto arrivato, non è più cumulative. Un'altra differenza è che i pacchetti non ordinati non vengono buttati, ma salvati per essere usati quando arrivano tutti quelli mancanti (e viene fatto l'acknowledgment). Importante! la window size deve essere larga al massimo quanto la metà del massimo sequence number, altrimenti si incorre in problemi in quanto non si capisce se un pacchetto è nuovo o è una retrasmissione.

Un problema che capita nella rete è che potrebbe arrivare un pacchetto vecchissimo con lo stesso sequence number di uno nuovo che sta arrivando. Come faccio a scartarlo? Per esempio, TCP decide che un pacchetto può vivere al massimo 3 minuti nella rete.

\b{Demultiplexing}: uguale a UDP; l'unica differenza è che un socket TCP viene identificato univocamente da quattro valori: source IP e port number, destination IP e port number. Inoltre, la porta 12000 (di solito) è la \b{welcoming port}, da dove iniziano tutte le connessioni TCP. Una volta connesso a quel socket, ci si mette d'accordo su quale porta usare.

\b{TCP}: offre un servizio \b{full-duplex}, cioè se A manda dati a B, anche B può mandare dati ad A. Inoltre, è \b{point-to-point}, cioè una connessione è solo fra una singola sorgente e una singola destinazione.

Il segmento TCP è strutturato in questo modo: 32-bit: sequence number e acknowledgment number. 16-bit: source port, destination port, receive window (flow control), checksum, urgent data(pointer all'ultimo byte dell'urgent data). 4-bit: header length. 1-bit: ACK (se sto mandando un acknowledgment), RST, SYN, FIN (tutti e tre flag di connessione, PSH(il receiver deve mandare subito i dati all'application layer), URG(flag per dati urgenti).

TCP usa i sequence e acknowledgment number non in base al numero del pacchetto, ma in base al numero del byte. Gli acknowledgment number sono un pochino strani: rappresentano il byte che mi aspetto che arrivi dall'altro host. Inoltre, gli acknowledgments sono \b{cumulative}. Tuttavia, se arrivano pacchetti in un ordine sbagliato, TCP non definisce nessuna politica precisa: si possono sia buttare sia tenere aspettando che arrivino quelli mancanti (molto spesso la seconda politica è quella più diffusa)

Importante! TCP non usa \b{NAK}!!

TCP usa i timeout per rimandare i pacchetti, e la durata viene decisa attraverso la stima del \b{round-trip-time}: \t{EstimatedRTT = $(1 - \alpha) \cdot $ EstimatedRTT $ + \alpha \quad \cdot $ SampleRTT}. Di solito vale che $\alpha = 0.125$. Inoltre, abbiamo anche una stima della differenza tra \t{EstimatedRTT} e \t{SampleRTT}, che si calcola come \t{DevRTT = $(1 - \beta) \cdot $ DevRTT $ + \beta \quad \cdot | $ SampleRTT - EstimatedRTT $ | $}. Alla fine, il timeout interval si calcola come \t{TimeoutInterval = EstimatedRTT + $4 \cdot $ DevRTT}. All'inizio, di solito, è settato a 1.

Modifiche possibili al TCP: \b{doubling timeout interval}: dopo un evento di timeout expiration, subito dopo aver rimandato il pacchetto con il seq number più piccolo non ancora confermato, il timeout interval viene raddoppiato, per evitare congestione. Infatti, nella maggior parte dei casi il timeout avviene a causa della congestione. \b{Fast retransmit}: se un sender riceve tre acknowledgment dello stesso dato già inviato, allora rimanda tutto senza aspettare il timeout. Questo succede perchè siccome TCP non ha \t{NAK}, vuol dire che c'è stato un errore e quel tipo particolare di dato non è proprio arrivato.

Fondamentalmente, TCP è un Go-Back-N protocol, ma con alcune grosse differenze: non è detto che butta i pacchetti non in ordine; se un accade un timeout expiration TCP rimanda solo quel pacchetto invece che tutti quanti quelli di cui non ha ricevuto un \t{ACK}

\b{Flow Control}: ogni host che intrattiene una connessione TCP si salva dei buffer per i dati della connessione. Il buffer del ricevente serve per esempio a tenersi i dati mentre l'application layer li legge. Il flow control serve a non riempire quel buffer, mandando i dati alla stessa velocità con cui l'applicazione li legge. Il sender si tiene una \b{receive window}, che è uguale allo spazio libero del buffer dove salva i dati il ricevente. La receive window non deve mai arrivare a 0. Per fare questo, basta essere sicuri che \t{LastByteSent - LastByteAcked $\leq$ ReceiveWindow}. Nel caso la ReceiveWindow dovesse arrivare a 0, allora il sender è autorizzato a inviare pacchetti grossi un byte per ricevere \t{ACK} per sapere se si è liberata.

\b{TCP connection}: 1) Il client manda un segment vuoto con la flag SYN attiva, e un valore nel sequence number random. 2) Il server riceve, crea i buffer e le variabili, e poi manda un segmento vuoto al client con la flag SYN attiva e il sequence number ricevuto più uno come acknowledgment number. Nel suo sequence number viene messo un valore a caso scelto dal server. Questo si chiama \b{SYNACK segment}, poichè significa che il server accetta la connessione. 3) Quando riceve il SYNACK, il client crea buffer e variabili. Poi rimanda al server un altro segment al server, con il sequence number scelto dal server più uno nell'acknowledgment number, e con la flag SYN a 0, dato che la connessione è ormai attiva. Questo terzo segment può già contenere dati. Questo processo si chiama \b{three-way handshake}. Per spegnere una connessione, il client (o il server) deve mandare un segment con il FIN a 1, e attendere l'\t{ACK} del server. Poi anche il server risponde con un segment con il FIN a 1, e aspettare l'\t{ACK} del client. A questo punto, tutte le risorse possono essere deallocate.

Gli stati che una connessione TCP (client) può avere sono: CLOSED, SYN\_SENT, ESTABLISHED, FIN\_WAIT\_1 (aspetto ack del mio FIN), FIN\_WAIT\_2 (aspetto il suo FIN), TIME\_WAIT ((ri)mando ack del suo FIN ricevuto). TIME\_WAIT no è infinito, si disattiva da solo dopo circa trenta secondi o un minuto.

Gli stati che un server può avere sono: CLOSED(server spento), LISTEN (aspetto connessioni), SYN\_RCVD, ESTABLISHED, CLOSE\_WAIT(ho ricevuto un FIN e mando il mio), LAST\_ACK (aspetto \t{ACK} del mio FIN).

\b{Principi di congestion control}: Scenario 1: la congestione avviene nel caso il throughput del sender è vicino a R/2, dove R è la capacità del link. Si incorre in delay molto alti a causa delle queue nel router. Scenario 2: alcuni pacchetti devono essere reinviati dal sender, e nel router il buffer è finito. Se il sender spara moltissimi pacchetti, il router ne perde molti a causa del suo buffer limitato, e il sender ne deve ritrasmetter altrettanti. Quindi si perde tempo a mandare in giro copie inutili di pacchetti. Scenario 3: un router deve essere condiviso da due connessioni. Magari però una di queste due connessioni è più vicina, e manda dati più velocemente. Piano piano, il router si riempie di dati di questa stessa. Tutti i pacchetti dell'altra, che arrivano da lontano, vengono buttati. Tutti la strada che hanno fatto fino a quel momento è stata inutile. La seconda connessione non parlerà mai.

Ci sono due modi principali di gestire il congestion control: End-to-End, come quello usato dal TCP, dove il network layer non da nessuna informazione riguardo la congestione, e se la devono sbrigare gli host tra di loro; Network-assited, dove il network layer comunica a ogni host lo stato della congestione. 

\b{TCP congestion control}: TCP ha una variabile, la \b{congestion window}, messa in modo tale che \t{LastByteSent - LastByteAcked $\leq$ min\{CongestionWindow, ReceiveWindow\}}. Fatta in questo modo, l'output rate del sender può essere approssimato come (CongestionWindow) / RTT byte/sec. Come fa tuttavia ad accorgersi di un \b{packet loss}? Semplice: o scade un timeout, oppure riceve tre \t{ACK} duplicati. 

\b{Slow start}: all'inizio di una connessione, vale che CongestionWindow = 1 MSS (dove MSS è la grandezza massima di un pacchetto). Ogni \t{ACK} ricevuto, la CongestionWindow viene aumentata di 1 MSS. In questo modo, ogni RTT, la grandezza di CongestionWindow raddoppia. Tre modi in cui smette di crescere, il primo: appena avviene un packet loss, la CongestionWindow viene resettata a 1 e ricomincia il processo, viene tuttavia salvato il valore (CongestionWindow / 2) in \t{sstresh}; secondo modo: quando CongestionWindow è ad un valore maggiore o uguale a sstresh, slow start finisce e si passa a congestion avoidance; terzo modo: il sender riceve tre \t{ACK} duplicati, e deve fare una fast retransmit, e passa allo stato Fast Recovery. 

\b{Congestion avoidance}: In questo stato, ogni acknowledgment fa aumentare solo di 1 la CongestionWindow. Quando mi accorgo di un packet loss, resetto CongestionWindow a 1 e riaggiorno sstresh alla metà del valore di CongestionWindow precedente, e passo allo stato Slow Start. Se invece riceve 3 \t{ACK} duplicati, allora dimezzo il valore di CongestionWindow e aggiorno sstresh, ed entro nello stato Fast Recovery. 

\b{Fast Recovery}: il valore di CongestionWindow aumenta di 1 MSS per ogni \t{ACK} duplicato ricevuto. Quando arriva il primo nuovo \t{ACK}, allora abbassa CongestionWindow e ritorna nello stato Congestion Avoidance. Se ottengo un packet loss per timeout, allora resetto CongestionWindow a 1, riaggiorno sstresh e ritorno a Slow Start. Da ricordare che Fast Recovery non è strettamente necessaria al funzionamento di TCP.

Assumendo che i packet loss vengono indicati da 3 \t{ACK} duplicati invece che da un timeout, il TCP congestion control viene definito una \b{additive-increase, multiplicative-decrease (AIMD)} forma di congestion control

Le connessioni TCP sono "fair", nel senso che si autobilanciano. Due connessioni TCP sullo stesso link prima o poi raggiungeranno la stessa velocità di throughput.

Spesso un trucco che viene usato per mandare più velocemente dati è usare multiple connessioni TCP parallele: questo gioca a vantaggio di chi manda i dati perchè le connessioni TCP si autobilanciano, quindi se ne ho più di una posso "imbrogliare"

\b{Forwarding} Ogni \b{router} ha la sua forwarding table. Il servizio network di internet prende il nome di \b{Best-Effort Service}. Ci sono due modi nel network layer di passarsi dati: sia with connections che connectionless (la stessa differenza che ci sta nel transport layer tra le connessioni TCP e UDP connectionless). Per le connessioni servono processi di handshaking etc. Per implementarle, serve una \b{Virtual Circuit Network}. Ci sta un path per arrivare da ogni router a un altro. Un pacchetto che appartiene a una Virtual Circuit ha un VC number nel suo header. La forwarding table di ogni router consiste in una tabella a cui a ogni possibile VC number dei pacchetti in arrivo corrisponde un'interfaccia di uscita (output link) e un VC number di output. Infatti, il VC number cambia nel corso del viaggio di un pacchetto. Questo è perchè altrimenti ci si impiegherebbe troppo tempo a mettersi d'accordo in tutta la rete su quali VC number usare. Inoltre, ogni router di una VC network deve tenersi le varie connection state information. Per formare una connessione in una VC network si deve passare attraverso 3 fasi: VC setup (comunico a chi mi voglio connettere; aspetto che la VC network crei un path per me), Data Transfer, VC Teardown (comunico che termino la connessione; la VC network aggiorna tutte le forwarding tables coinvolte di eliminare il path). L'unica grande differenza dal sistema di connessioni TCP è che mentre nel TCP solo due end system sono al cosciente di una particolare connessione, nelle VC tutti i router coinvolti in un path hanno le informazioni di quella connessione. I messaggi usati per configurare una VC si chiamano signaling messages, e i protocolli si chiamano signaling protocols.

\b{Datagram Network}: Niente path, niente connessioni. Un end system mette l'indirizzo di un certo pacchetto nel suo header e poi lo butta nel network, e non deve più preoccuparsene. Si è sviluppata così tanto per la sua semplicità di implementazione, e per la facilità con cui diverse tecnologie possono essere collegate fra loro.

\b{Longest Prefix Matching}: Nei router la forwarding table è formata da vari prefix, e a ognuno corrisponde un'interfaccia di uscita. Quando arriva un pacchetto, il router ne esamina l'indirizzo bit per bit, e lo manda attraverso l'interfaccia a cui corrisponde il longest prefix match.

Un router è composto da varie input ports, da varie output ports (che possono essere anche collegate se il router è bidirezionale), da una switching fabric, e dal routing processor. L'unica parte che esegue software è il routing processor, mentre tutto il resto fa parte del \b{forwarding}, implementato a livello hardware. Ogni input port ha la sua copia della forwarding table, e appena arriva un pacchetto fa un lookup hardware velocissimo (a volte anche usando cache ram) e capisce in quale output port va mandato. Dopo il pacchetto viene inoltrato nella switching fabric (se è libera). Il routing processor si occupa di aggiornare la forwarding table, con tutte le operazioni necessarie a farlo (calcolare percorsi etc)
La \b{switching fabric} è fatta in 3 modi: via memory, dove cioè un pacchetto viene scritto in una memoria, e poi il routing processor lo manda in una output port (ma non deve fare calcoli che sono tutti già stati fatti nelle input port), via bus, dove cioè a ogni pacchetto viene messo un mini-header che indica quale output port usare, poi viene mandato su un bus che lo inoltra a tutte le output port, e solo quella giusta lo propaga (tuttavia sul bus ci può stare solo un pacchetto alla volta), e via interconnection network, dove cioè ci sta una rete interconnessa di bus con switch che si aprono e chiudono a seconda della strada che deve fare il pacchetto.

Sia le input che le output ports hanno dei buffer per salvarsi i pacchetti arrivati o da mandare. Se questi buffer si riempiono, si occorre in packet loss. La lunghezza di questi buffer di solito è c*rtt, dove c è la capacità del link e rtt è l'average round trip time. Ci sono vari algoritmi per decidere quale pacchetto servire nella queue dei pacchetti (non è sempre FIFO, a volte si usano complessi metodi per garantire uguale servizio a tutti i vari host comunicanti).

\b{Datagram}: composto da vari cambi, come version number, header length (per la lunghezza delle opzioni), type of service (real time oppure no), datagram length (lunghezza header + data), identifier, flags, fragmentation offset, time to live (se arriva a 0 il pacchetto viene buttato), protocol (6 per TCP, 17 per UDP), header checksum, source IP, destination IP, options (di varia lunghezza), e alla fine data, cioè il payload. Senza opzioni, l'header del datagram occupa 20 byte.
\b{Fragmentation}: Siccome nel percorso che fa un pacchetto passa attraverso vari e diversi link protocol, potrebbe succedere che uno di essi abbia il MTU (maximum transmission unit) molto basso. Quindi, a volte un datagram è troppo grosso per essere mandato. Viene diviso in due o più datagram più piccoli, e mandato. Vengono riassemblaty all'end system di destinazione. Ogni datagram inviato ha un identifier number, quindi se due pacchetti hanno lo stesso identifier vuol dire che sono parte di uno frammentato. L'ordine dei frammenti viene ripristinato grazie al fragmentation offset. L'ultimo frammento ha una flag posta a 0, mentre tutti gli altri ce l'hanno settata a 1.

\b{IP addressing}: ogni interfaccia (cioè uscita ad un link) di ogni host e di ogni router deve avere un IP unico e globale. Un network che connette degli host ad un router, o dei router tra di loro, si chiama \b{subnet}. Secondo la strategia Classless Interdomain Routing (CIDR), un IP ha la forma \t{a.b.c.d\textbackslash x}, dove \t{x} indica il numero di bit del prefisso, e questo prefisso indica la subnet della destinazione voluta. In questo modo, i router qunado devono propagare un pacchetto guardano solo il prefisso; il resto dei bit vengono guardati quando si deve decidere in quale host della subnet tocca mandare il pacchetto. L'unico indirizzo speciale è 255.255.255.255, che indica come destinazione tutti gli host della subnet corrente. Si occupa di gestire gli indirizzi IP la Internet Corporation for Assigned Names and Numbers (ICANN)

\b{DHCP}: Il protocollo Dynamic Host Configuration Protocol serve a comunicare a un host che si connette a un router varie informazioni sulla rete, tra cui il suo indirizzo IP (che può essere fisso o temporaneo), l'indirizzo del DNS locale, la subnet mask etc. E' un protocollo plug and play, che funziona in automatico, fatto apposta per le situazioni in cui ci sono molti utenti che vanno e vengono, in una rete dinamica. Ci sono quattro step: DHCP server discovery (l'host manda un DHCP discover message con un pacchetto UDP alla porta 67, all'indirizzo 255.255.255.255), DHCP server offer (il server DHCP risponde con un DHCP offer message all'indirizzo 255.255.255.255, con dentro un IP valido e l'IP address lease time, cioè il tempo per cui è valido), DHCP request (l'host decide se l'IP ricevuto va bene, poichè potrebbero anche essere più di uno, e risponde all'offerta con un DHCP request message), e infine DHCP ACK (il server risponde alla DHCP request con un DHCP ACK message). Ci sono anche altri modi, per comunicare ad esempio che voglio continuare ad usare il mio IP oltre il lease time consentito.

\b{NAT}: Netword Address Translation: se una subnet diventa molto più grande del previsto, finiscono presto gli IP disponibili. Allora, si usa una NAT: in pratica tutta la subnet, per internet, prende un'unico indirizzo IP che punta al router. Dentro la subnet, si usano degli indirizzi IP privati, che non hanno significato all'esterno della rete domestica. Ora, il router NAT quando deve mandare un pacchetto all'esterno, usa una porta diversa per ogni porta uscente da ogni host, e si salva quale porta ha usato. Quando riceve un pacchetto dall'esterno, controlla la porta a cui è indirizzato e lo rimanda all'host e alla porta corrispondenti ai dati che si era salvato. Questo porta alcuni vantaggi come l'enorme numero di host che si possono connettere (65,000, come i numeri delle porte), ma anche vari svantaggi, soprattutto il fatto che gli host sono meno visibili nella rete e non possono comunicare attraverso porte prefissate, quindi gli vengono impediti una serie di servizi come connessioni TCP (che devono essere ottenute attraverso NAT traversal) oppure l'impossibilità di funzionare come server. Per aggirare questo problema si può usare \b{UPnP} (universal plug and play), in cui chiedo al NAT di mappare una precisa porta pubblica per una certa porta di un certo host.

\b{ICMP}: Internet Control Message Protocol: sono pacchetti speciali portati dentro l'IP payload. In pratica nei layer è un pochino sopra IP. I messaggi ICMP hanno un campo tipo e un campo codice, e contengono l'header e i primi 8 byte del datagram che ha generato il messaggio ICMP. Per esempio Traceroute funziona mandando messaggi UDP con porte sbagliate e aspettando di ricevere messaggi ICMP di errore.

\b{IPv6}: Principali differenze: gli indirizzi passano da 32 bit a 128 bit; aggiunti gli anycast address, in cui si richiede che un messaggio venga mandato a uno qualunque di un gruppo di host; header fisso a 40 byte. Inoltre, adesso il formato del pacchetto IPv6 diventa fatto dai seguenti campi: version, traffic class (come type of service in IPv4), flow label, payload length, next header (come il protcol field in IPv4), hop limit (se arriva a 0 il pacchetto viene buttato), source address, destination address, e data, cioè il payload.
Non ci sta più la frammentazione: se un pacchetto è troppo grosso per essere inviato attaverso un certo link, allora viene rimandato un ICMP message con scritto "Packet too big" ed è proprio il source host a mandare datagram più piccoli. E' stato rimossa la checksum, dato che sia UDP che TCP la fanno per conto loro, e inoltre sarebbe dovuta stata ricalcolata a ogni router, dato che dipende dal campo time to live. Sono state eliminate le opzioni. Vengono trattate come un payload, e se ci sono indicate dal campo next-header.

\b{IPv4 to IPv6}: Ci sono due modi principali per passare da IPv4 a IPv6 dappertutto: uno è il dual-stack, in cui i router che sono doppiamente compatibili, se vedono che un hop va fatto attraverso un nodo IPv4, semplicemente convertono l'header (perdendo alcune informazioni come il flow), e poi l'IPv6 viene ripristinato come possibile da un nodo IPv6. L'altro è il tunneling, in cui se si deve passare attraverso un nodo IPv4 si incapsula il messaggio IPv6 dentro un IPv4, e viene decapsulato appena possibile da un nodo IPv6. Per capire quali destinazioni accettano IPv6 e quali no, ce lo dice il DNS.

Di solito un host è connesso a un solo router, detto il \b{default router} ( o anche detto il first-hop router). Allora, questo router, per mandare pacchetti ad altri router, deve usare algoritmi di instradamento. Possono avere varie caratteristiche: possono essere \b{globali} (calcolo del cammino su tutta la rete, detti algoritmi link-state), \b{decentralizzati} (calcolo del cammino solo nella mia parte del grafo, poi parlo con i vicini, detti algoritmi distance-vector), \b{statici} (i cammini cambiano molto raramente), \b{dinamici} (varia sia il grafo sia il volume del traffico), \b{sensibili al carico} (i costi dei collegamenti variano dinamicamente in base al traffico), \b{insensibili} (non cambiano i costi).

\b{Algoritmo d'instradamento link-state (LS)}: ogni nodo conosce tutto il grafo, questo lo otteniamo facendo scambiare tra i nodi dei pacchetti di stato, che contengono collegamenti e costi vari. Ottenuto tramite un algoritmo di link-state broadcast. In pratica viene usato l'algoritmo di Dijstra. I percorsi vengono ricostruiti in base ai predecessori di ogni nodo. La complessità è $O(n^2)$, se si usa un heap il tempo minimo scende a logaritmico.

\b{Algoritmo d'instradamento distance-vector (DV)}: è un algoritmo \b{distribuito}, \b{iterativo}, \b{asincrono}. Ogni nodo mantiene le seguenti informazioni: il costo del cammino per ciascun vicino, il vettore distanza, e il vettore distanza di ciascuno dei suoi vicini. Quando un nodo riceve un nuovo vettore distanza da qualcuno dei suoi vicini, lo salva e usa la formula di Bellman-Ford per aggiornare il proprio vettore distanza $D_x(y) = min_v \{ c(x, v) + D_v(y) \}$ per ciascun nodo y in N. I vettori distanza vengono scambiati tra i nodi all'inizio e ogni qual volta dovesse cambiare un collegamento. \b{Reverse poisoning}: se un nodo $z$ deve passare attraverso $y$ per raggiungere $x$, allora nel distance vector che passa a $y$ mentirà dicendo che la sua distanza a $x$ è infinita. In questo modo, $y$ non tenterà mai di passare attraverso $z$ per arrivare a $x$.  Questo serve a prevenire \b{routing loop}, che avviene quando viene aumentato il costo di un collegamento (finchè non vengono riaggiornate le forwarding table, un pacchetto potrebbe rimbalzare all'infinito tra due nodi). Tuttavia, il reverse poisoning riesce a risolvere routing loops in cui sono coinvolti due soli nodi; per quelli che coinvolgono più nodi non c'è soluzione al count to infinity problem.

\b{Count-to-infinity}: si verifica quando in una rete del tipo A-B-C viene perso il collegamento con A. C, che non è ancora al corrente della situazione, sostiene di poter arrivare ad A in due passi (attraverso B). B, siccome non p più arrivare ad A, allora decide di mandarlo a C credendo che lui ci possa arrivare, e così via.

\b{LS vs DV routing algorithm}: Inanzitutto, nel LS la complessità dei messaggi passati è enorme, in quanto devo passare informazioni su tutto il grafo ($O(N \cdot E)$ messaggi passati in giro). Tuttavia, nel DV la velocità di convergenza è pessima: mentre nel LS richiede tempo $O(N^2)$ per eseguire Dijkstra, in DV non è garantito che si raggiunga questa convergenza (risultato ottimale), e in caso favorevole ci si arriva molto lentamente. Per quanto riguarda la robustezza, vince LS poichè viene influenzato meno dagli errori (in DV potrebbe passare molto tempo prima che la rete si accorga di un router malfunzionante). Questi due algoritmi sono i più usati, ci sono altri algoritmi di routing del tipo \b{circuit switched routing} ma poco usati e in casi molto particolari.

\b{Autonomous systems}: gli algoritmi di routing visti finora funzionano solo per reti piccole e limitate (e soprattuto prive di una disposizione gerarchica); nella realtà i router vengono raggruppati in autonomous system, all'interno dei quali vengono eseguiti questi algoritmi di routing per distribuire i pacchetti. L'algoritmo che gestisce il traffico dentro un AS è chiamato \b{intra-AS routing protocol} (simile ai due visti prima). Ogni AS possiede alcuni router capaci di comunicare con altri AS, e questi prendono il nome di \b{gateway routers}. Invece, il protocollo che si occupa di ottenere informazioni dagli AS interconnessi e propagare l'informazione a tutti i router interni all'AS prende il nome di \b{inter-AS routing protocol}. Affinchè due AS possano comunicare, devono usare lo stesso protocollo. In internet, tutti gli AS usano lo stesso inter-AS protocol, chiamato BGP4.

\b{Hot potato routing}: Un router dentro un AS, quando deve mandare un pacchetto all'esterno dell'AS (e quindi deve scegliere come destinazione uno dei vari gateway router dell'AS), semplicemente lo manda al gateway router più vicino (cioè quello per il quale il pacchetto impiega meno tempo).

\b{Routing Information Protocol}: RIP è uno dei primi protocolli intra-AS, ed è molto simile all'algoritmo distance vector. Invece delle distanze usa il numero di hop che il sorgente deve fare per arrivare a destinazione. Gli hop non sono contati attraverso i router, ma attraverso il numero di sottoreti attraversate (destinazoine compresa). Il massimo numero di hop consentito è 15. Ogni 30 secondi, i router si scambiano aggiornamenti sullo stato della rete, chiamati RIP response messages (o b{RIP advertisements}). Ogni router tiene la sua \b{routing table}, che contiene sia la forwarding table, che il distance vector. Per ogni destinazione, oltre al numero di hop necessari, nella routing table c'è anche scritto qual è il primo router a cui va mandato un pacchetto indirizzato a una certa destinazione. Se dopo 180 secondi non ho più ricevuto notizie da un router, lo considero irraggiungibile. Importante: RIP in realtà è implementato nell'application layer protocol: i RIP advertisements vengono trasportati attraverso UDP nella porta 520 (quindi dietro ogni router che esegue RIP ci deve essere una macchina UNIX che esegue il processo \b{routed}).

\b{OSPF}: considerato il successore di RIP, è simile a un algoritmo Link-State. Ogni router esegue Dijkstra imponendo se stesso come radice. Inoltre, ogni router manda informazioni a tutti gli altri router dell'AS (flooding), invece che solo ai vicini. Manda informazioni ogni qual volta c'è stato un cambiamento in un link, oppure periodicamente ogni 30 minuti. I messaggi OSPF (\b{OSPF advertisements}) sono trasportati direttamente da IP (quindi il protocollo deve implementare da solo reliable transfer etc). Importante: i costi dei collegamenti in un AS vengono settati a mano dal network administrator (spesso sono inversamente proporzionali alla capacità del link, in modo che un link lento ha un peso molto alto). I vantaggi principali di OSPF sono: Sicurezza (gli advertisements sono autenticabili, attraverso password semplici oppure anche attraverso hash MD5 sia del pacchetto che della password); Multiple Same Cost Paths (se per arrivare a una certa destinazine ci sono vari percorsi di costo uguale, OSPF permette di distribuire i pacchetti attraverso i vari percorsi); Integrated Support for Unicast and Multicast routing; Hierarchy Support (un AS puo' venire diviso in aree, ognuna che usa localmente la sua versione di OSPF, e un'area particolare, la backbone, collega tutte le aree fra loro).

\b{Border Gateway Protocol} (BGP4): In questo protocollo coppie di router si scambiano informazioni attraverso connessioni TCP semipermanenti sulla porta 179. Questi due router prendono il nome di \b{BGP peers}, mentre la connessione si chiama \b{BGP session}. Se la connessione avviene tra due router dello stesso AS, allora è una internal BGP session (iBGP); altrimenti, è una external BGP session (eBGP). Un AS puo' fare l'advertising delle sue subnet attraverso il più lungo prefisso comune. In BGP, ogni AS è identificato da un unico autonomous system number (ASN). Quando un router fa un advertisement BGP, non manda solo i prefissi raggiungibili dal suo AS, ma manda anche una serie di attributi, che insieme ai prefissi prendono il nome di \b{route}. I più importanti attributi sono: AS-PATH (contiene tutti gli AS attraverso cui l'advertisement è passato. Usato per prevenire looping); NEXT-HOP (interfaccia del router che inizia l'AS-PATH. Usato per configurare correttamente le forwarding table). \b{Import policy}: quando una particolare route arriva ad un router, il router puo' decidere se scartarla o no, oppure settare certi attributi. Questo poichè spesso un certo router non è autorizzato a mandare messaggi che passano attraverso alcuni particolari AS (che appartengono a compagnie diverse). \b{Route selection}: se ci sono più route possibili, allora BGP sceglie in base ai seguenti criteri, in ordine:  Local Preference (settato dal network administrator), shortest AS-PATH, closest NEXT-HOP (intra-AS hot potato), BGP identifiers.

\b{Broadcast Routing}: un singolo nodo source vuole mandare un certo pacchetto a tutta la rete. L'approccio più semplice (N-way unicast) consiste nel mandare una copia del pacchetto a ogni destinazione. Non solo è molto inefficiente (intaso molto il primo link che attraverso), ma è anche difficile e richiederebbe protocolli aggiuntivi (come fa il sorgente a conoscere tutte le destinazioni?). Un altro approccio è uncontrolled flooding, in cui quando un router riceve un pacchetto broadcast, manda una copia del pacchetto a tutti i suoi vicini (tranne a quello da cui è arrivato il pacchetto); tuttavia ci sta il problema enorme del \b{broadcast storm}: non solo verranno creati trilioni di pacchetti, ma se nel grafo c'è un ciclo allora i pacchetti non smetteranno mai di girare. Invece, nel controlled flooding, si usa un approccio chiamato \b{reverse path forwarding}: quando un router riceve un pacchetto broadcast con un indirizzo di sorgente, trasmette il pacchetto a tutti i vicini se e solo se il pacchetto è arrivato dal link che sta sul percorso più corto dal router in questione alla sorgente. Previene looping e broadcast storm. Un altro approccio un po' più debole è il \b{sequence number controlled flooding}: il sorgente scrive il suo indirizzo e un broadcast sequence number nel pacchetto, e lo manda a tutti i vicini. Ogni router a questo punto si tiene una lista, per ogni sorgente, dei pacchetti broadcast arrivati, così evita di mandare due volte un pacchetto che è già arrivato in precedenza. Al posto del flooding, ci sta un'altra tecnica molto elegante: lo \b{Spanning-Tree Broadcast}: nel viene costruito un minimum spanning tree. Ogni nodo manda il pacchetto broadcast solo attraverso i link che fanno parte del minimum spanning tree. In questo modo, ogni nodo riceve il pacchetto broadcast una e una sola volta. Come si costruisce questo spanning tree? \b{center-based approach}: viene preso un nodoo centrale, e tutti gli altri nodi mandano un pacchetto verso questo nodo centrale. Il pacchetto si ferma quando arriva al nodo centrale, o quando incontra unnodo che sta già sullo spanning tree, e determina su quale ramo dello spanning tree il nodo che ha inviato il pacchetto sta.

\b{Multicast Routing}: un pacchetto viene mandato da una sorgente a un subset dei nodi della rete. I subset sono gestiti come multicast group, dove ogni gruppo ha un suo preciso indirizzo IP, in modo tale che se voglio mandare un pacchetto a tutto il gruppo devo solo specificare quell'indirizzo, e non tutti gli indirizzi singoli di tutte le destinazioni. \b{Internet Group Management Protocol}: in pratica fa comunicare un host con il suo first-hop router, nel caso l'host voglia far parte di un certo multicast group. Ha solo tre tipi di messaggi: membership\_query (il router chiede agli host su un'interfaccia di quali gruppi fanno parte); membership\_report (risposta al membership query da parte dell'host; puo' anche essere usato dall'host quando l'host entra in un group e vuole notificarlo al router); leave\_group (è opzionale, in quanto il router capisce se un host non sta più in un gruppo se non risponde più al membership\_query dopo un certo timeout). E' un esempio di soft-state (stato che termina dopo un certo timeout). Tuttavia, per mandare messaggi tra i multicast, servono altri algoritmi, i \b{multicast routing algorithms} (poichè IGMP fa parlare solo l'host con il suo first-hop router). Il punto è che si deve costruire un albero nella rete che collega tutti gli host di un certo multicast group (questo albero potrebbe comprendere anche router di passaggio che non fanno parte del gruppo). Ci sono due approcci fondamentali: Multicast routing con group-shared tree, dove viene costruito un solo albero da un nodo centrale nella stessa maniera con cui viene costruito uno spanning tree (ma in cui rimane il problema di come scegliere il nodo centrale), e Multicast routing con source-based tree, dove ogni sorgente ha un suo albero che costruisce traimite RPF (reverse path forwarding), anche se è necessaria una tecnica chiamata pruning per evitare che nodi che non fanno parte del multicast ricevano troppi pacchetti RPF per la costruzione dell'albero. Tuttavia nell'applicazione reale l'algoritmo più usato è il Protocol Independent Multicast (PIM) routing protocol, in cui fa differenza in base alla vicinanza degli utenti di un certo gruppo. In modalità densa, PIM usa flood-and-prune RPF. In modalità sparse, PIM crea vari rendezvous points per creare l'albero. 

I servizi che il link layer può offrire sono: Framing (incapsulamento del datagram in un frame), Link Access (un medium access protocol - MAC - specifica le regole con cui un frame viene trasmesso in un link), Reliable Delivery (garanzia del corretto trasporto dei bit; tuttavia, spesso i cavi hanno error rate talmente basso che non c'è bisogno di questa garanzia), Error detection e Error correction.

Il link layer è implementato spesso in un \b{network adapter}, anche chiamato \b{network interface card (NIC)}. Quindi la gran parte dei servizi del link layer sono implementati a livello hardware.

\b{Parity Checks}: la più semplice forma di parity check è il parity bit (conto il numero di uni e scrivo se è pari o no), che ha una probabilità di beccare l'errore attorno al 50\%. Se invece uso il \b{two-dimensional parity}, divido il messaggio in righe e colonne, e per ogni colonna uso il parity bit. Se ci sta un solo errore, lo posso anche correggere, mentre due errori li posso solo detectare.
\b{Checksumming}: si fa il complemento a 1 della somma dei byte e si ottiene il checksum. Il ricevente somma tutti i byte al checksum e dovrebbe ottenere un numero con tutti 1. Tuttavia, è una forma abbastanza debole di error checking, infatti viene usata solo a livello software da TCP e cose varie. Il link layer puo' implementare a livello hardware cose come il \b{Cyclic Redundancy Check} (CRC): sia il sender che il receiver si accordano su $r + 1$ bit chiamati il generatore, dove il primo bit è sempre 1. Quando il sender vuole mandare $d$ bits, prende altri $r$ bits in modo tale che il numero formato appendendo $R$ a $D$ sia divisibile per il generatore in aritmetica modulo 2. Il ricevente deve solo controllare che il nuero ricevuto (formato da $d + r$ bits) sia divisibile per il generatore. CRC è capace di detectare tutti gli errori che coinvolgono fino a $r$ bits; inoltre, c'è buona probabilità di trovare errori anche più lunghi.

Ci sono due tipi di link: i \b{point-to-point link} (una singola sorgente, una sola destinazione), e i \b{broadcast link} (molte sorgenti e destinazioni connesse allo stesso broadcast channel, come Wi-Fi o Ethernet). Questi ultimi si chiamano broadcast perchè ogni pacchetto viene inviato a tutti i nodi.

\b{Channel Partitioning Protocols}: TDM (time-division multiplexing), in pratica vengono assegnati N time slots, uno per ogni host, e ogni host puo' inviare pacchetti nel suo slot, molto fair (tutti gli host ottengono bitrate di R/N), ma se ci sta un solo host che vuole parlare è molto svantaggiato; FDM (frequency-division multiplexing), il canale viene diviso in piccole frequenze, in modo che ogni host ha R/N bitrate, ma stessi vantaggi e svantaggi di TDM; CDMA (code division multiple access), ogni nodo ha un codice con cui cripta i propri pacchetti.

\b{Random Access Protocols}: Tutti i node mandano alla massima bitrate. Se due pacchetti collidono, allora i due nodi coinvolti li riinviano aspettando un delay random.
\b{Slotted Aloha}: Tutti i frame sono composti da L bits. Il tempo è diviso in slots da L/R secondi (cioè giusto il tempo per trasmettere un frame). I nodi iniziano a trasmettere frame solo all'inizio di uno slot.. Se due pacchetti di due nodi collidono, i due nodi se ne accorgono prima che lo slot finisca. Ogni nodo coinvolto in una collisione manda il pacchetto incriminato nel prossimo slot con probabilità $p$. E così via per tanti slot, fino a che non lo manda senza collisioni. Il vantaggio principale è che è un protocollo molto semplice e veloce (quando ci sono pochi nodi). Tuttavia, se i nodi sono tanti, l'efficienza si riduce drasticamente, in quanto uno slot è buono solo se ci scrive un nodo solo (quindi probabilità di uno slot buono = $Np(1-p)^{N-1}$). Si dimostra che l'efficienza massima è del 37\%.
\b{Aloha}: Come prima, solo che non ci sono gli slot. In pratica mando un frame appena posso. Se ho una collisione, con probabilità $p$ lo mando, altrimenti aspetto un frame transmission time e riprovo con probabilità $p$. Siccome per una trasmissione per essere buona devono aver finito tutti di trasmettere (con prob $(1-p)^{N-1}$), e nessuno deve iniziare a trasmettere durante la trasmissione (con prob $(1-p)^{N-1}$), allora la probabilità di una trasmissione buona è $p(1-p)^{2(N-1)}$, e l'efficienza è esattamente la metà di slotted aloha.
\b{CSMA}: (Carrier Sense Multiple Access): un nodo trasmette se e solo se, ascoltando il canale, nessun altro sta trasmettendo. Tuttavia si incorre lo stesso in collisioni per il channel propagation delay (il temp o che ci mette un segnale per arrivare da un nodo all'altro). \b{Collision Detection}: Se un nodo CSMA, mentre trasmette, si accorge di una collisione (cioè riceve un segnale da un altro nodo che sta trasmettendo), allora smette di trasmettere, e aspetta un intervallo random per rimandare tutto il frame. Per scegliere l'intervallo si usa il binary exponential backoff (se un particolare frame ha colliso $n$ volte, allora deve aspettare un intervallo preso a caso tra $\{ 0 \ldots 2^n - 1 \}$, moltiplicato per un valore che per esempio in Ethernet è 512 bit times). L'efficienza del Collision Detection è data dalla formula $1 / (1 + 5(d / t))$, dove $d$ è il propagation delay e $t$ è il tempo che ci vuole a mandare un frame.

\b{Taking Turns Protocol}: Sia ALOHA che CSMA hanno la proprietà che se ci sta un solo nodo nel canale, quello raggiunge una velocità di R bits, ma non garantiscono che se ci sono tanti nodi la velocità media sarà di R/N bits. Vediamo se alcuni protocolli la possono garantire: \b{Polling Protocol}: un nodo diventa il master node, che dice a ogni altro nodo, a turno, che ha la possibilità di mandare al massimo un certo numero di frame. Quando quel nodo ha finito (e lo si capisce dall'assenza di segnale), il master da il permesso a un altro nodo, e così via. Svantaggi: polling delay (il tempo che si deve aspettare perchè il master dia i permessi, e quindi se c'è un solo nodo attivo non riesce ad ottenere R bits di velocità), e il problema che se per qualche motivo il master fallisce la rete non funziona più.
\b{Token-Passing Protocol}: ci sta un frame particolare, il token, che serve a decidere chi parla. Quando un nodo ha finito di trasmettere, passa il token al prossimo nodo. Ci sta un numero massimo di frame che un nodo puo' mandare prima di passare il token. Se un nodo fallisce, cade la rete. Se un nodo si rifiuta di passare il token, si perde il token e ci stanno delle procedure per rimetterlo in circolazione.

\b{Indirizzi link-layer}: un indirizzo link-layer è anche chiamato un LAN address, un Physical address o un MAC address. Ogni adapter (network interface) di un host o un router ha un suo MAC address (proprio come avrebbe un suo IP address). Tuttavia, i link switches non hanno un loro address. Per la maggior parte delle LAN il MAC address è lungo 6 byte, spesso espresso in esadecimale. Inoltre, ogni adapter ha un unico MAC address (responsabilità della IEEE e dei manufattori di adapters. Nella maggior parte dei casi, un indirizzo MAC è fisso, non cambia in base al luogo. Spesso un frame viene mandato in broadcast, e ogni adpater per capire se è destinato a lui oppure no confronta il MAC address del frame con il suo proprio MAC address. L'indirizzo usato per mandare un frame a tutti quanti nella LAN è FF-FF-FF-FF-FF-FF. Il MAC address è stato introdotto per rendere il protocollo layer indipendente dal protocollo network (e rendere più veloci le cose)
\b{Address Resolution Protocol} (ARP) : Serve a ottenere il MAC address di un'interfaccia sapendo il suo IP. Simile al DNS, ma funziona solo localmente in una subnet al posto che in tutto il mondo. In pratica ogni router e host ha una sua ARP table, dove a ogni IP corrisponde un MAC address e un TTL (time to live, di solito 20 minuti). Se devo mandare un pacchetto a un'indirizzo noto, bene; altrimenti, mando nella subnet un ARP packet, un pacchetto speciale che viene mandato in broadcast, in cui chiedo chi ha questo particolare IP. L'host che ha quel particolare IP risponde con un altro ARP packet (questa volta non in broadcast, ma indirizzato proprio a me), e io aggiorno la mia ARP table. Il protocollo ARP funziona tutto in automatico, non ha bisogno di manutenzione da parte del network administrator. Se voglio mandare un frame fuori dalla mia subnet, allora il MAC address che dovro' usare è quello del mio first-hop-router, e poi lui penserà a fare il resto del lavoro per me.

\b{Ethernet}: All'inizio di ethernet, tutti gli host e router sono connessi a un hub con twisted-pair copper wire, usando CSMA/CD come multiple access protocol. In realtà, è un protocollo sia link che fisico. Un hub è un dispositivo che appena riceve un bit da un interfaccia, lo ritrasmette con più potenza a tutte le altre interfacce (quindi fa da broadcast LAN, tutti ricevono tutti i pacchetti). Un frame ethernet è composto da: Data field (da 46 a 1500 byte, se va oltre deve essere frammentato da IP, altrimenti deve essere riempito con roba inutile), Source address e Destination address (6+6 byte), Type Field (2 byte, per capire il tipo di network layer protocol da usare, come per esempio IP oppure anche ARP, anche se non sarebbe proprio un network protocol), Cyclic Redundancy Check (4 byte), Preamble (8 byte, i primi 7 sono 10101010, mentre l'ultimo è 10101011, per sincronizzare gli orologi tra sender e receiver). Ethernet è connectionless, e non offre reliable transport. Nei primi anni 2000 gli hub sono stati sostituiti con i \b{switch}. I switch sono trasparenti agli host, nel senso che non vengono percepiti nella rete, ma fanno il loro lavoro in silenzio. Ogni switch ha la sua switch table, dove a ogni MAC address corrisponde una certa interfaccia e l'istante in cui quella tupla è stata inserita nella tabella. Non è detto che la tabella contenga tutti gli indirizzi della sottorete. Grazie alla tabella un switch fa il filtering (drop dei pacchetti non necessari) e il forwarding (mandare i pacchetti all'interfaccia giusta). Quando arriva un pacchetto al switch, ci sono tre casi: il switch non conosce l'indirizzo MAC di destinazione (in questo caso manda il pacchetto a tutte le interfacce tranne quella da cui è provenuto il pacchetto); il pacchetto arriva dalla stessa interfaccia attraverso cui il switch avrebbe dovuto fare il forwarding (in questo caso il switch droppa il pacchetto); il pacchetto arriva da un'interfaccia diversa (in questo caso il switch fa il forwarding del pacchetto sull'interfaccia giusta presa dalla switch table). Una caratteristica importante dei switch è che non devono essere configurati in nessun modo, in quanto la switch table si costruisce da sola: all'inizio è vuota, e si riempie salvandosi da quale interfaccia è arrivato un pacchetto con un certo MAC address; un'entrata nella tabella viene cancellata se passa troppo tempo dall'ultimo pacchetto arrivato. Quindi, i switch sono plug-and-play, in quanto funzionano interamente da soli. Inoltre, sono full-duplex, in quanto ogni interfaccia puo' ricevere e inviare allo stesso tempo. Importante: ogni switch ha un buffer per salvarsi i frame da mandare, proprio come i router. I vantaggi dei switch: nessuna collisione (i switch si salvano i frame nei buffer e mandano un solo frame alla volta su un'interfaccia, quindi non ci sarebbe neanche bisogno di un multiple access control protocol), link eterogenei (si possono collegare tecnologie diverse, non è necessaria una unica tecnologia di broadcast), più sicuri (se si rompe un cavo si rompe solo quella particolare connessione invece che tutta la subnet, e inoltre eventuali ospiti indesiderati ricevono molti meno pacchetti in broadcast). In fin dei conti, in molti casi un switch potrebbe sostituire un router, in quanto è plug-and-play ed è più veloce a fare forwarding; tuttavia, un router puo' calcolare i percorsi più corti per i pacchetti ed è invulnerabile ad attacchi di broadcast storm. Ci sono un altro tipo di dispositivi usati da Ethernet: i \b{bridges}, che sono come dei switch ma con due porte sole (o coswitchesmunque molto poche): hanno principalmente la funzione di filtering (cioè non fanno passare un pacchetto il cui MAC address non sta nella zona in cui sta andando), e quindi riescono a dividere la subnet in collision domains; usano bridge tables, costruite e usate nello stesso modo delle switch tables (self-learning). Tuttavia funzionano bene solo quando non ci sono dei cicli nel grafo, quindi devono essere organizzati secondo uno spanning tree, ma questo comporta anche uno svantaggio in quanto non si possono usare path alternativi per ravvelocizzare lo scambio di pacchetti. Sono molto veloci ma non offrono protezione da broadcast storms. Per il resto tutte le altre caratteristiche sono uguali ai switch.

\b{Wireless Things}: gli wireless hosts si connettono attraverso wireless links alle base stations. Se sono in infrastructure mode, allora tutti i servizi di network comuni (come DNS, routing...) vengono formiti dalla base station; altrimenti si dice che sono ad hoc networks. Quando un host mobile cambia base station, fa un processo chiamato handoff. Sostituendo un link fisico con uno wireless, ottengo le seguenti modifiche: decrescita della potenza del segnale, interferenza da sorgenti estranee, multipath propagation (parti delle onde elettromagnetiche rimbalzano su alcune superfici prendendo percorsi diversi e arrivando in tempi diversi; peggio ancora se nell'ambiente ci sono oggetti che si muovono). Per misurare la qualità di un segnale wireless si usa la Signal-To-Noise-Ratio (SNR), misurata in dB, che sostanzialmente venti volte il log-10 del rapporto fra l'ampiezza del segnale e l'ampiezza del noise. Dato un certo SNR, aumentare la velocità di trasmissione di bit aumenta il Bit-Error-Rate (BER), la probabilità che un bit venga sbagliato. Per questo spesso il segnale viene modulato e la velocità di trasmissione è dinamica, a differenza dei link wired. Due problemi dei collegamenti wireless sono: hidden terminal problem (un terminale riceve due segnali broadcast da due terminali solo che uno dei due è nascosto all'altro da un ostacolo e non vengono rilevate collisioni) e il fading (un terminale riceve due broadcast da due terminali diversi che sono pero' troppo lontani per accorgersi della loro collisione)

\b{Code Division Multiple Access} (CDMA): appartiene alla famiglia dei channel partitioning protocols. In pratica ogni bit mandato viene codificato con un codice che cambia molto più velocemente del bitrate (a una velocità chiamata chipping rate). Un bit viene mandato in un certo intervallo di tempo chiamato bit slot, e questo viene diviso in $M$ mini bit slots. Il codice è lungo $M$, e si ripete a ogni bit. Il ricevente, per riottenere la sequenza di bit originaria, rimoltipica tutti i mini bit slots ricevuti per il codice, li somma tutti e li divide per $M$; facendo questo per ogni bit slot dovrebbe ottenere 0 o 1. Incredibilmente questa cosa funziona anche se ci sono vari mittenti e si sommano i messaggi trasmessi. Infatti, se i codici vengono scelti bene (ovviamente diversi per ogni mittente), il ricevente puo' fare lo stesso processo e riottenere le varie sequenze originarie di bit usando i vari codici.

\b{Carrier Sense Multiple Access with Collision Avoidance} (CSMA/CA) : appartiene alla famiglia dei random access protocols. Come in Ethernet, ogni station sta a sentire il canale prima di trasmettere. Non si usa la collision detection (smetto di trasmettere se capisco che qualcun altro sta trasmettendo), poichè è molto complicato capire se un altro sta trasmettendo (per la scarsa potenza del segnale, e per problemi come hidden terminal problem e fading); infatti qui, quando inizio a mandare un frame, non interrompo più, ma lo invio per intero. Funziona così: all'inzio, se il canale è idle (nessuno sta parlando) aspetto un intervallo di tempo chiamato Distributed Inter-frame Space; altrimenti, scelgo un random backoff value usando la binary exponential backoff (come in Ethernet) e faccio decrescere questo backoff solo mentre il canale è idle; quando il backoff arriva a zero, mando la mia frame e aspetto il mio ack (tutte le station, quando ricevono un frame che supera il controllo CRC, aspettano un piccolo intervallo chiamato Short Inter-frame Spacing e mandano indietro un ack frame); appena arriva l'ack, ricomincio dal punto 2 (scelgo un backoff etc.); se l'ack non arriva proprio, ritorno al punto 2 aumentando l'intervallo di scelta del backoff. La grande differenza dal CSMA/CD è che il backoff time decresce solo quando il canale è idle, invece che continuamente. Questo accade perchè come è detto prima è molto difficile accorgersi delle collisioni, e quindi è molto meglio provare direttamente a evitarle. CONTINUARE QUI.

\b{IEEE 802.11} wireless LAN (WiFi): ci sono tre versioni principali di 802.11 (a, b, g; la prima a 11 Mbps, le altre due a 54 Mbps). Inoltre, una nuova, la 802.11n, usa varie antenne sia sul mittente che sul ricevente, per mandare segnali contemporaneamente. Una basic service set (BSS) contiene una o più wireless stations e una base station centrale, chiamata access point (AP). Prima di cominciare a trasmettere dati, una wireless station si deve connettere ad un AP. Un AP ha un Service Set Identified (SSID), un nome formato da una o due parole, e un channel number (siccome 802.11 funziona nella banda 2.4 - 2.485 GHz, ci sono 85 MHz su cui scegliere un canale; 802.11 definisce 11 canali parzialmente sovrapposti; due canali non si sovrappongono se sono separati da almeno quattro canali. Ogni AP, inoltre, manda periodicamente un beacon frame, che contiene lo SSID e il MAC address dell'AP. Per capire quali AP sono accessibili in un certo posto, una wireless station fa lo scan degli 11 canali aspettando questi benedetti beacon frames con le dita incrociate (attenzione: non è detto che due AP diversi trasmettano su due canali diversi), e questo processo si chiama passive scanning. Invece, una wireless station puo' anche mandare un probe broadcast a cui tutti gli AP rispondono; questo invece prende il nome di active scanning. Dopo aver scelto un AP a cui collegarsi, mando una association request, a cui l'AP risponde con una association response. Una volta associato, mando un DHCP discovery message per ottenere il mio IP e per collegarmi finalmente alla subnet corrispondente. Se necessaria autenticazione, questa avviene attraverso o una whitelist di MAC addresses, oppure attraverso username/password (a volte il processo di autenticazione è decentralizzato a un authentication server che funziona per vari AP).  Una WiFi jungle è un posto fisico dove una wireless station riceve segnale abbastanza potente da almeno due AP.

{\huge manca IPsec, DOCSIS, versioni di ethernet, PPP?}

\end{document}
